{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b468ace8-8b67-4b06-b242-28c119db8a87",
   "metadata": {},
   "source": [
    "THIS SECTION IS PREP AND PRETESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb783c74-b114-4d89-aedf-576e225f122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works for CPU\n",
    "#!pip install -U transformers\n",
    "#!pip install -U accelerate\n",
    "#!pip install -U datasets\n",
    "#!pip install -U bertviz\n",
    "#!pip install -U umap-learn\n",
    "#!pip install seaborn --upgrade\n",
    "#!pip install evaluate\n",
    "#!pip install -U openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683dcab1-391d-4049-bc7d-9326bfff2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extras when using GPU\n",
    "#!pip install nvidia_smi\n",
    "#!pip install pip install pynvml\n",
    "#import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#the pip install over nvidia_smi gave this warning. the above line is to follow these instructions\n",
    "    #huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "    #To disable this warning, you can either:\n",
    "    #\t- Avoid using `tokenizers` before the fork if possible\n",
    "    #\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8bb797-838a-4356-8436-30026325b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA version: 12.4\n",
      "Is CUDA available: True\n",
      "CUDA device name: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU detected'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abfae217-dec0-46ac-84f0-75c1c81a8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Monitoring has started: 15:25:50\n",
      "GPU Monitoring has ended: 15:27:18\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import nvidia_smi\n",
    "monitor_script_path = \"/home/charlie/Desktop/Practical/GPU_Measuring/GPU_Performance2.py\"\n",
    "\n",
    "monitor_process = subprocess.Popen(['python', monitor_script_path])\n",
    "print(f\"GPU Monitoring has started: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "#Larger matrix means more gpu utilisation\n",
    "matrix_size=3000\n",
    "Loop_Amount=5000\n",
    "a = torch.randn(matrix_size, matrix_size, device='cuda')\n",
    "for _ in range(Loop_Amount):  \n",
    "    b = torch.matmul(a, a)\n",
    "\n",
    "monitor_process.terminate() \n",
    "print(f\"GPU Monitoring has ended: {time.strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d54bd-f3c3-4a33-bcf8-325d76a25881",
   "metadata": {},
   "source": [
    "THE ACTUAL PROCESS STARTS FROM THE CELL BELOW ONWARDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3467cd3-32d7-4a62-86b0-c4c5eccdf770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 19)\n",
      "label\n",
      "0    100000\n",
      "1    100000\n",
      "Name: count, dtype: int64\n",
      "             ts                 uid      id.orig_h  id.orig_p      id.resp_h  \\\n",
      "0  1.547145e+09  COwxOt46psNUp7lXxd  192.168.1.198      10803  75.212.242.95   \n",
      "1  1.547145e+09  Ch4Syi18QvtG30zVLf  192.168.1.198      10803  96.229.181.62   \n",
      "\n",
      "   id.resp_p proto service  duration  orig_bytes  resp_bytes conn_state  \\\n",
      "0         23   tcp     NaN       NaN         NaN         NaN         S0   \n",
      "1         23   tcp     NaN       NaN         NaN         NaN         S0   \n",
      "\n",
      "   missed_bytes history  orig_pkts  orig_ip_bytes  resp_pkts  resp_ip_bytes  \\\n",
      "0             0       S          1             40          0              0   \n",
      "1             0       S          1             40          0              0   \n",
      "\n",
      "   label  \n",
      "0      0  \n",
      "1      0  \n",
      "Successfully loaded all dataframes\n",
      "Cell Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18779/2851146783.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label']=df['label'].replace({\"Benign\":0,\"Malicious\":1})#Converting all Benign to 0 and Malcious to 1\n"
     ]
    }
   ],
   "source": [
    "#Alternatively if it can load all dataframes without crashing\n",
    "import pandas as pd\n",
    "\n",
    "input_file = '/home/charlie/Desktop/Data_Subset/Masterfile/subset_2.csv'\n",
    "chunk_size = 10000\n",
    "\n",
    "try:\n",
    "    df = pd.concat(pd.read_csv(input_file, chunksize=chunk_size))\n",
    "    dropped_columns=[\"local_orig\", \"local_resp\", \"tunnel_parents\", \"detailed-label\"]\n",
    "    df.drop(columns=dropped_columns, inplace=True)\n",
    "    df['label']=df['label'].replace({\"Benign\":0,\"Malicious\":1})#Converting all Benign to 0 and Malcious to 1\n",
    "    print(df.shape)\n",
    "    print(df['label'].value_counts())\n",
    "    print(df[:2])\n",
    "    print(\"Successfully loaded all dataframes\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5711900d-ecaa-42fc-a415-3c26fdf322ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rows,columns)\n",
      "\n",
      "Data Frame:(200000, 19)\n",
      "Training data:(140000, 19)\n",
      "Test data:(40000, 19)\n",
      "validation data:(20000, 19)\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "#splitting data\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#below specified 0.3 of df is seperated to test, with a distrubtion of label column\n",
    "train, test = train_test_split(df,test_size=0.3,stratify=df['label'])\n",
    "#creates validation set by taking 1/3 of test\n",
    "test, validation= train_test_split(test, test_size=1/3,stratify=test['label'])\n",
    "\n",
    "print(\"(rows,columns)\\n\")\n",
    "print(f\"Data Frame:{df.shape}\")\n",
    "print(f\"Training data:{train.shape}\")\n",
    "print(f\"Test data:{test.shape}\")\n",
    "print(f\"validation data:{validation.shape}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8612afcd-b9a1-4e92-ae84-e79dfdd9868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'label', '__index_level_0__'],\n",
      "        num_rows: 140000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'label', '__index_level_0__'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'label', '__index_level_0__'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "#convert dataframes in dataset dictionary objects, uses chunking to work with large datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "chunk_size = 10000\n",
    "try:\n",
    "    def df_chunking(df, chunk_size=chunk_size):\n",
    "        for start in range(0, len(df),chunk_size):\n",
    "            yield Dataset.from_pandas(df.iloc[start:start + chunk_size],preserve_index=True)\n",
    "\n",
    "    #make each dataframe split a chunk \n",
    "    train_chunks= list(df_chunking(train))\n",
    "    test_chunks= list(df_chunking(test))\n",
    "    validation_chunks= list(df_chunking(validation))\n",
    "\n",
    "    #Concatenates each dataframe chunk into datasets\n",
    "    train_dataset = concatenate_datasets(train_chunks)\n",
    "    test_dataset = concatenate_datasets(test_chunks)\n",
    "    validation_dataset = concatenate_datasets(validation_chunks)\n",
    "\n",
    "\n",
    "\n",
    "    #makes data dictionaries\n",
    "    dataset= DatasetDict(\n",
    "        { \"train\": train_dataset,\n",
    "          \"test\": test_dataset,\n",
    "          \"validation\": validation_dataset\n",
    "        }\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "print(f\"dataset:{dataset}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09d063e8-9387-4355-826f-149ffb2bb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 65 new tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30575, 768, padding_idx=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New token test\n",
    "# Import necessary modules\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  \n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\") \n",
    "\n",
    "# new tokens\n",
    "new_token = [\n",
    "    \"1533042911.474174\", \"C5JLGOoxIw2dBZt47\", \"192.168.100.113,123\",\n",
    "    \"81.2.254.224,123\", \"udp\", \"\", \"0.00549,48.0\", \"48.0\", \"SF\", \"\", \"\",\n",
    "    \"0\", \"Dd\", \"1\", \"76\", \"1\", \"76\", \"(empty)\", \"Benign\", \"-\", \n",
    "    \"1536241151.35095\", \"C9m6XdPw0pRf5lNa1\", \"192.168.100.111\", \n",
    "    \"18344,156.212.84.219\", \"52869\", \"tcp\", \"\", \"4e-06\", \"0.0\", \"0.0\", \n",
    "    \"S0\", \"\", \"\", \"0\", \"S\", \"2\", \"80\", \"0\", \"0\", \"-\", \"Malicious\", \n",
    "    \"DDoS\", \"1536232599.053273\", \"C0lARQ2hM1UUe4aSrg\", \"192.168.100.111\", \n",
    "    \"17832\", \"156.112.51.76\", \"37215\", \"tcp\", \"\", \"1e-06\", \"0.0\", \"0.0\", \n",
    "    \"S0\", \"\", \"\", \"0\", \"S\", \"2\", \"80\", \"0\", \"0\", \"-\", \"Malicious\", \n",
    "    \"Okiru\", \"1536229808.84214\", \"CTL3WP1XmST9PFud2\", \"192.168.100.111\", \n",
    "    \"17576\", \"53.178.3.247\", \"8081\", \"tcp\", \"\", \"1e-06\", \"0.0\", \"0.0\", \n",
    "    \"S0\", \"\", \"\", \"0\", \"S\", \"2\", \"80\", \"0\", \"0\", \"-\", \"Malicious\", \n",
    "    \"PartOfAHorizontalPortScan\", \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \n",
    "    \"id.resp_h\", \"id.resp_p\", \"proto\", \"service\", \"duration\", \n",
    "    \"orig_bytes\", \"resp_bytes\", \"conn_state\", \"local_orig\", \"local_resp\", \n",
    "    \"missed_bytes\", \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \n",
    "    \"resp_ip_bytes\", \"tunnel_parents\", \"label\", \"detailed-label\"\n",
    "]\n",
    "\n",
    "# Add tokens to tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(new_token)\n",
    "print(f\"Added {num_added_tokens} new tokens.\")\n",
    "\n",
    "# Resize the token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76f57ef9-234a-4470-85a5-888e393327e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get info on distilberts tokeniser\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint=\"distilbert-base-uncased\"\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "distilbert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd0a30d5-3504-4311-80a6-8310ab112a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1533042911.474174', ',', 'c5jlgooxiw2dbzt47', ',', '192.168.100.113,123', ',', '81.2.254.224,123', ',', 'udp', ',', ',', '0.00549,48.0', ',', '48.0', ',', 'sf', ',', ',', ',', '0', ',', 'dd', ',', '1', ',', '76', ',', '1', ',', '76', ',', '(empty)', ',', 'benign', ',', '-']\n"
     ]
    }
   ],
   "source": [
    "# Testing how tokenisation went\n",
    "input_sample=\"1533042911.474174,C5JLGOoxIw2dBZt47,192.168.100.113,123,81.2.254.224,123,udp,,0.00549,48.0,48.0,SF,,,0,Dd,1,76,1,76,(empty),Benign,-\"\n",
    "\n",
    "\n",
    "# Tokenize the input sample\n",
    "tokens = tokenizer.tokenize(input_sample)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8640dfad-c4a2-4cfd-b36d-c1646cf11ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1031, 1015, 5139, 2581, 1015, 3429, 1014, 1014, 30540, 6255, 1016, 5345, 1010, 1015, 5139, 2581, 1015, 3429, 1014, 1015, 1016, 1012, 6421, 1016, 1014, 4293, 1033, 1031, 1005, 18133, 2100, 3511, 2509, 18418, 28483, 9148, 13668, 4160, 1016, 1005, 1010, 1005, 1039, 2595, 2278, 2078, 2620, 2050, 1015, 1043, 3501, 5051, 2683, 6784, 3669, 2226, 1016, 1005, 1033, 1031, 1005, 1015, 1023, 1016, 1012, 1015, 6273, 1012, 1015, 1012, 1015, 5818, 1005, 1010, 1005, 1015, 1023, 1016, 1012, 1015, 6273, 1012, 1015, 1012, 1015, 5818, 1005, 1033, 1024, 1031, 1016, 1018, 1015, 1014, 1018, 1010, 1016, 6584, 1014, 1018, 1033, 1011, 1028, 1031, 1005, 1015, 6146, 1012, 1015, 3486, 1012, 1015, 1014, 1022, 1012, 1016, 1018, 1015, 1005, 1010, 1005, 6640, 1012, 1015, 1016, 1020, 1012, 1015, 4466, 1012, 1015, 1020, 1015, 1005, 1033, 1024, 1031, 1016, 1017, 1010, 1016, 1017, 1033, 1031, 1005, 30538, 1005, 1010, 1005, 30538, 1005, 1033, 1031, 3904, 1010, 3904, 1033, 9367, 1024, 1031, 3904, 1010, 3904, 1033, 30563, 1024, 1031, 3904, 1010, 3904, 1033, 30564, 1024, 1031, 3904, 1010, 3904, 1033, 30565, 1024, 1031, 1005, 30541, 1005, 1010, 1005, 30541, 1005, 1033, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139776/139776 [00:36<00:00, 3836.86 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39936/39936 [00:09<00:00, 3999.93 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19968/19968 [00:05<00:00, 3910.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#convert any strings to numeric representations\n",
    "import torch\n",
    "try: \n",
    "    def tokenize(batch):\n",
    "        log_entry = (\n",
    "            f\"{batch['ts']} {batch['uid']} {batch['id.orig_h']}:{batch['id.orig_p']} -> \"\n",
    "            f\"{batch['id.resp_h']}:{batch['id.resp_p']} {batch['proto']} {batch['service']} \"\n",
    "            f\"duration: {batch['duration']} orig_bytes: {batch['orig_bytes']} resp_bytes: {batch['resp_bytes']} \"\n",
    "            f\"conn_state: {batch['conn_state']}\"\n",
    "        )\n",
    "        return tokenizer(log_entry, padding=\"max_length\", truncation=True,max_length=512)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "sample = dataset['train'][:2] \n",
    "print(tokenize(sample))  \n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "#model_max_length=512 learned this from cell to evlaute distilbert tokeniser, so batch_size = 512\n",
    "encoded_dataset = dataset.map(tokenize, batched=True, batch_size=512, drop_last_batch=True)\n",
    "#Drops last batch as last batch isn't 512 causing crash of cell, ask supervisor\n",
    "print(\"Cell Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5df09f81-80b1-4a01-9949-46306b81a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (139776, 22)\n",
      "Label distribution:\n",
      "label\n",
      "0    69889\n",
      "1    69887\n",
      "Name: count, dtype: int64\n",
      "Successfully loaded all dataframes\n"
     ]
    }
   ],
   "source": [
    "# Compare dropped batch and see if the balance is lost\n",
    "df = encoded_dataset['train'].to_pandas()\n",
    "\n",
    "# Print shape and label distribution\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts()) \n",
    "print(\"Successfully loaded all dataframes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f4191da-c9ac-4de0-9810-9577988d645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'Benign': 0, 'Malicious': 1}\n",
      "Model: distilbert-base-uncased\n",
      "Computer: cuda\n",
      "Model Config /n\n",
      ": DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"Benign\": 0,\n",
      "    \"Malicious\": 1\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"0\": \"Benign\",\n",
      "    \"1\": \"Malicious\"\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "#Loads model and passess config for classifier\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "\n",
    "label2id={\"Benign\":0,\"Malicious\":1}\n",
    "id2label={0:\"Benign\",1:\"Malicious\"}\n",
    "model_checkpoint=\"distilbert-base-uncased\"\n",
    "\n",
    "\n",
    "num_labels=len(label2id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(model_checkpoint,label2id=id2label,id2label=label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config).to(device)\n",
    "\n",
    "print(f\"Labels: {model.config.id2label}\")\n",
    "print(f\"Model: {model_checkpoint}\")\n",
    "print(f\"Computer: {device}\")\n",
    "print(\"Model Config /n\")\n",
    "print(f\": {config}\")\n",
    "print(\"Cell Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7006642-eb4e-40bc-b2bd-f12accead54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "# Build compute metrics function\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics_evaluate(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4174d73-bbef-42a6-b476-0b493955b522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arugment: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=train_dir/runs/Jan13_17-44-44_Asclepius,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=train_dir,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=train_dir,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n",
      "Cell Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training arguement \n",
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "training_dir = \"train_dir\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                                  output_dir=training_dir,\n",
    "                                  overwrite_output_dir = True,\n",
    "                                  num_train_epochs = 2,\n",
    "                                  learning_rate = 2e-5,\n",
    "                                  per_device_train_batch_size = batch_size,\n",
    "                                  per_device_eval_batch_size = batch_size,\n",
    "                                  weight_decay = 0.01,\n",
    "                                  evaluation_strategy = 'epoch',\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Training Arugment: {training_args}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b313cee8-1488-4ac0-aadd-33efc9c9daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18779/3748774873.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics_evaluate,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=distilbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "242c4dc8-7ea2-479f-b9a5-8ac5179d719f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2172\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2173\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2174\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2175\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2176\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/trainer.py:3678\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3676\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3677\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3678\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[1;32m   3680\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3683\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3684\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/trainer.py:3734\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3732\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3733\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3734\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:977\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 977\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[1;32m    978\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    979\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    980\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    981\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    982\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    983\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    984\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    985\u001b[0m )\n\u001b[1;32m    986\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    987\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:784\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    782\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 784\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_flash_attention_2:\n\u001b[1;32m    787\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask \u001b[38;5;28;01mif\u001b[39;00m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attention_mask) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:131\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids, input_embeds)\u001b[0m\n\u001b[1;32m    127\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m input_embeds \u001b[38;5;241m+\u001b[39m position_embeddings  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021261e8-b564-4fcc-8524-07154d8a2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "# error above \"RuntimeError: The size of tensor a (8) must match the size of tensor b (512) at non-singleton dimension 1\"\n",
    "print(f\"Shape of input_embeds: {input_embeds.shape if input_embeds is not None else 'None'}\")\n",
    "print(f\"Shape of input_ids: {input_ids.shape if input_ids is not None else 'None'}\")\n",
    "print(f\"Shape of position_embeddings: {position_embeddings.shape}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
