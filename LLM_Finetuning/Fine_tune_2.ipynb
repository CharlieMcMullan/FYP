{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b468ace8-8b67-4b06-b242-28c119db8a87",
   "metadata": {},
   "source": [
    "THIS SECTION IS PREP AND PRETESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb783c74-b114-4d89-aedf-576e225f122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This works for CPU\n",
    "#!pip install -U transformers\n",
    "#!pip install -U accelerate\n",
    "#!pip install -U datasets\n",
    "#!pip install -U bertviz\n",
    "#!pip install -U umap-learn\n",
    "#!pip install seaborn --upgrade\n",
    "#!pip install evaluate\n",
    "#!pip install -U openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "683dcab1-391d-4049-bc7d-9326bfff2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extras when using GPU\n",
    "#!pip install nvidia_smi\n",
    "#!pip install pip install pynvml\n",
    "#import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#the pip install over nvidia_smi gave this warning. the above line is to follow these instructions\n",
    "    #huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
    "    #To disable this warning, you can either:\n",
    "    #\t- Avoid using `tokenizers` before the fork if possible\n",
    "    #\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8bb797-838a-4356-8436-30026325b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA version: 12.4\n",
      "Is CUDA available: True\n",
      "CUDA device name: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU detected'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abfae217-dec0-46ac-84f0-75c1c81a8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Monitoring has started: 16:42:48\n",
      "GPU Monitoring has ended: 16:44:17\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import nvidia_smi\n",
    "monitor_script_path = \"/home/charlie/Desktop/Practical/GPU_Measuring/GPU_Performance2.py\"\n",
    "\n",
    "monitor_process = subprocess.Popen(['python', monitor_script_path])\n",
    "print(f\"GPU Monitoring has started: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "#Larger matrix means more gpu utilisation\n",
    "matrix_size=3000\n",
    "Loop_Amount=5000\n",
    "a = torch.randn(matrix_size, matrix_size, device='cuda')\n",
    "for _ in range(Loop_Amount):  \n",
    "    b = torch.matmul(a, a)\n",
    "\n",
    "monitor_process.terminate() \n",
    "print(f\"GPU Monitoring has ended: {time.strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d54bd-f3c3-4a33-bcf8-325d76a25881",
   "metadata": {},
   "source": [
    "THE ACTUAL PROCESS STARTS FROM THE CELL BELOW ONWARDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3467cd3-32d7-4a62-86b0-c4c5eccdf770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12800, 19)\n",
      "label\n",
      "0    6400\n",
      "1    6400\n",
      "Name: count, dtype: int64\n",
      "             ts                 uid      id.orig_h  id.orig_p  \\\n",
      "0  1.536233e+09  CMidSA4riIrgA8iWfj  156.249.40.90          3   \n",
      "1  1.536233e+09   C03NhMRUsZDAoHhyg    12.89.9.154          3   \n",
      "\n",
      "         id.resp_h  id.resp_p proto service  duration  orig_bytes  resp_bytes  \\\n",
      "0  192.168.100.111         10  icmp     NaN  0.000005        80.0         0.0   \n",
      "1  192.168.100.111         13  icmp     NaN  0.000005        56.0         0.0   \n",
      "\n",
      "  conn_state  missed_bytes history  orig_pkts  orig_ip_bytes  resp_pkts  \\\n",
      "0        OTH             0     NaN          2            136          0   \n",
      "1        OTH             0     NaN          2            112          0   \n",
      "\n",
      "   resp_ip_bytes  label  \n",
      "0              0      0  \n",
      "1              0      0  \n",
      "Successfully loaded all dataframes\n",
      "Cell Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18779/1903815677.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label']=df['label'].replace({\"Benign\":0,\"Malicious\":1})#Converting all Benign to 0 and Malcious to 1\n"
     ]
    }
   ],
   "source": [
    "#Alternatively if it can load all dataframes without crashing\n",
    "import pandas as pd\n",
    "\n",
    "input_file = '/home/charlie/Desktop/Data_Subset/Masterfile/subset_1.csv'\n",
    "chunk_size = 10000\n",
    "\n",
    "try:\n",
    "    df = pd.concat(pd.read_csv(input_file, chunksize=chunk_size))\n",
    "    dropped_columns=[\"local_orig\", \"local_resp\", \"tunnel_parents\", \"detailed-label\"]\n",
    "    df.drop(columns=dropped_columns, inplace=True)\n",
    "    df['label']=df['label'].replace({\"Benign\":0,\"Malicious\":1})#Converting all Benign to 0 and Malcious to 1\n",
    "    print(df.shape)\n",
    "    print(df['label'].value_counts())\n",
    "    print(df[:2])\n",
    "    print(\"Successfully loaded all dataframes\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5711900d-ecaa-42fc-a415-3c26fdf322ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(rows,columns)\n",
      "\n",
      "Data Frame:(12800, 19)\n",
      "Training data:(8960, 19)\n",
      "Test data:(2560, 19)\n",
      "validation data:(1280, 19)\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "#splitting data\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#below specified 0.3 of df is seperated to test, with a distrubtion of label column\n",
    "train, test = train_test_split(df,test_size=0.3,stratify=df['label'])\n",
    "#creates validation set by taking 1/3 of test\n",
    "test, validation= train_test_split(test, test_size=1/3,stratify=test['label'])\n",
    "\n",
    "print(\"(rows,columns)\\n\")\n",
    "print(f\"Data Frame:{df.shape}\")\n",
    "print(f\"Training data:{train.shape}\")\n",
    "print(f\"Test data:{test.shape}\")\n",
    "print(f\"validation data:{validation.shape}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8612afcd-b9a1-4e92-ae84-e79dfdd9868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/anaconda3/envs/Current_Env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'label', '__index_level_0__'],\n",
      "        num_rows: 8960\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'label', '__index_level_0__'],\n",
      "        num_rows: 2560\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'label', '__index_level_0__'],\n",
      "        num_rows: 1280\n",
      "    })\n",
      "})\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "#convert dataframes in dataset dictionary objects, uses chunking to work with large datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "\n",
    "chunk_size = 10000\n",
    "try:\n",
    "    def df_chunking(df, chunk_size=chunk_size):\n",
    "        for start in range(0, len(df),chunk_size):\n",
    "            yield Dataset.from_pandas(df.iloc[start:start + chunk_size],preserve_index=True)\n",
    "\n",
    "    #make each dataframe split a chunk \n",
    "    train_chunks= list(df_chunking(train))\n",
    "    test_chunks= list(df_chunking(test))\n",
    "    validation_chunks= list(df_chunking(validation))\n",
    "\n",
    "    #Concatenates each dataframe chunk into datasets\n",
    "    train_dataset = concatenate_datasets(train_chunks)\n",
    "    test_dataset = concatenate_datasets(test_chunks)\n",
    "    validation_dataset = concatenate_datasets(validation_chunks)\n",
    "\n",
    "\n",
    "\n",
    "    #makes data dictionaries\n",
    "    dataset= DatasetDict(\n",
    "        { \"train\": train_dataset,\n",
    "          \"test\": test_dataset,\n",
    "          \"validation\": validation_dataset\n",
    "        }\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "print(f\"dataset:{dataset}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f57ef9-234a-4470-85a5-888e393327e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get info on distilberts tokeniser\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint=\"distilbert-base-uncased\"\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "distilbert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8640dfad-c4a2-4cfd-b36d-c1646cf11ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sample:\n",
      "{'input_ids': [101, 16710, 2575, 19317, 19841, 23499, 1012, 4720, 28756, 2620, 2475, 1039, 2620, 11475, 2072, 2475, 2595, 2140, 2475, 2860, 2361, 2480, 2860, 22540, 3654, 17613, 1012, 16923, 1012, 2531, 1012, 11118, 1024, 10700, 2549, 1011, 1028, 19975, 1012, 8574, 1012, 13092, 1012, 10550, 1024, 4720, 20842, 2683, 22975, 2361, 3904, 9367, 1024, 1016, 2063, 1011, 5757, 2030, 8004, 1035, 27507, 1024, 1014, 1012, 1014, 24501, 2361, 1035, 27507, 1024, 1014, 1012, 1014, 9530, 2078, 1035, 2110, 1024, 1055, 2692, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8704/8704 [00:02<00:00, 3751.11 examples/s]\n",
      "Map: 100%|██████████| 2560/2560 [00:00<00:00, 3950.24 examples/s]\n",
      "Map: 100%|██████████| 1024/1024 [00:00<00:00, 3858.14 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert any strings to numeric representations\n",
    "import torch\n",
    "\n",
    "# Define the tokenization function\n",
    "try: \n",
    "    def tokenize(batch):\n",
    "        # Construct the log entry as a string\n",
    "        log_entry = (\n",
    "            f\"{batch['ts']} {batch['uid']} {batch['id.orig_h']}:{batch['id.orig_p']} -> \"\n",
    "            f\"{batch['id.resp_h']}:{batch['id.resp_p']} {batch['proto']} {batch['service']} \"\n",
    "            f\"duration: {batch['duration']} orig_bytes: {batch['orig_bytes']} resp_bytes: {batch['resp_bytes']} \"\n",
    "            f\"conn_state: {batch['conn_state']}\"\n",
    "        )\n",
    "        # Apply the Hugging Face tokenizer\n",
    "        return distilbert_tokenizer(log_entry, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "\n",
    "# Extract the first two rows (returns a dictionary of lists)\n",
    "sample = dataset['train'][:2]\n",
    "\n",
    "# Convert the first sample into a dictionary (for testing)\n",
    "first_sample = {key: sample[key][0] for key in sample}  # Extract the first log entry as a dict\n",
    "\n",
    "# Test the tokenization on a single sample\n",
    "print(f\"Tokenized sample:\\n{tokenize(first_sample)}\\n\")\n",
    "\n",
    "# Apply tokenization to the entire dataset\n",
    "# batched=True ensures that the function is applied to batches of rows\n",
    "encoded_dataset = dataset.map(tokenize, batched=True, batch_size=512, drop_last_batch=True)\n",
    "print(\"Cell Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "043ec794-f686-419d-9cc8-f2282717ac64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 16710, 2575, 19317, 19841, 23499, 1012, 4720, 28756, 2620, 2475, 1039, 2620, 11475, 2072, 2475, 2595, 2140, 2475, 2860, 2361, 2480, 2860, 22540, 3654, 17613, 1012, 16923, 1012, 2531, 1012, 11118, 1024, 10700, 2549, 1011, 1028, 19975, 1012, 8574, 1012, 13092, 1012, 10550, 1024, 4720, 20842, 2683, 22975, 2361, 3904, 9367, 1024, 1016, 2063, 1011, 5757, 2030, 8004, 1035, 27507, 1024, 1014, 1012, 1014, 24501, 2361, 1035, 27507, 1024, 1014, 1012, 1014, 9530, 2078, 1035, 2110, 1024, 1055, 2692, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(dataset['train'][0]))  # Run on the first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd5d074c-80c0-4acf-b4ae-6929dbee55b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL ERROR: object of type 'int' has no len()\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "try:\n",
    "\n",
    "\n",
    "    for i in range(5):\n",
    "        print(f\"Sample {i} length: {len(encoded_dataset['train'][i]['input_ids'])}\")\n",
    "\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40e586a6-3aea-4a4a-a261-77a25f2ba224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL ERROR: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "try:\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(encoded_dataset['train'], batch_size=8)\n",
    "    for batch in dataloader:\n",
    "        print(f\"Sample batch shape: {batch['input_ids'].shape}\")\n",
    "        break  # Check the\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5df09f81-80b1-4a01-9949-46306b81a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8704, 22)\n",
      "Label distribution:\n",
      "label\n",
      "0    4360\n",
      "1    4344\n",
      "Name: count, dtype: int64\n",
      "Successfully loaded all dataframes\n"
     ]
    }
   ],
   "source": [
    "# Compare dropped batch and see if the balance is lost\n",
    "df = encoded_dataset['train'].to_pandas()\n",
    "\n",
    "# Print shape and label distribution\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['label'].value_counts()) \n",
    "print(\"Successfully loaded all dataframes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f4191da-c9ac-4de0-9810-9577988d645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'Benign': 0, 'Malicious': 1}\n",
      "Model: distilbert-base-uncased\n",
      "Computer: cuda\n",
      "Model Config /n\n",
      ": DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"Benign\": 0,\n",
      "    \"Malicious\": 1\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"0\": \"Benign\",\n",
      "    \"1\": \"Malicious\"\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "#Loads model and passess config for classifier\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "\n",
    "label2id={\"Benign\":0,\"Malicious\":1}\n",
    "id2label={0:\"Benign\",1:\"Malicious\"}\n",
    "model_checkpoint=\"distilbert-base-uncased\"\n",
    "\n",
    "\n",
    "num_labels=len(label2id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = AutoConfig.from_pretrained(model_checkpoint,label2id=id2label,id2label=label2id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config).to(device)\n",
    "\n",
    "print(f\"Labels: {model.config.id2label}\")\n",
    "print(f\"Model: {model_checkpoint}\")\n",
    "print(f\"Computer: {device}\")\n",
    "print(\"Model Config /n\")\n",
    "print(f\": {config}\")\n",
    "print(\"Cell Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7006642-eb4e-40bc-b2bd-f12accead54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell Completed\n"
     ]
    }
   ],
   "source": [
    "# Build compute metrics function\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics_evaluate(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4174d73-bbef-42a6-b476-0b493955b522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arugment: TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=train_dir/runs/Jan13_17-37-35_Asclepius,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=train_dir,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=32,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=train_dir,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n",
      "Cell Completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/anaconda3/envs/Current_Env/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training arguement \n",
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "training_dir = \"train_dir\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                                  output_dir=training_dir,\n",
    "                                  overwrite_output_dir = True,\n",
    "                                  num_train_epochs = 2,\n",
    "                                  learning_rate = 2e-5,\n",
    "                                  per_device_train_batch_size = batch_size,\n",
    "                                  per_device_eval_batch_size = batch_size,\n",
    "                                  weight_decay = 0.01,\n",
    "                                  evaluation_strategy = 'epoch',\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Training Arugment: {training_args}\")\n",
    "print(\"Cell Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b313cee8-1488-4ac0-aadd-33efc9c9daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18779/3748774873.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics_evaluate,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['validation'],\n",
    "    tokenizer=distilbert_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "242c4dc8-7ea2-479f-b9a5-8ac5179d719f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL ERROR: The size of tensor a (8) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021261e8-b564-4fcc-8524-07154d8a2da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b86ef-57f4-4cae-b3a9-d3014072fc44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
