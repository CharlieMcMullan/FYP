{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0890f07-bd62-4c31-9fc6-a7d838a5cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA RTX A2000 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a9d36f-a342-46b0-b80f-15f60512f8ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nvidia_smi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnvidia_smi\u001b[39;00m\n\u001b[1;32m      5\u001b[0m monitor_script_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/charlie/Desktop/Practical/GPU_Measuring/GPU_Performance2.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nvidia_smi'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import nvidia_smi\n",
    "monitor_script_path = \"/home/charlie/Desktop/Practical/GPU_Measuring/GPU_Performance2.py\"\n",
    "\n",
    "monitor_process = subprocess.Popen(['python', monitor_script_path])\n",
    "print(f\"GPU Monitoring has started: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "#Larger matrix means more gpu utilisation\n",
    "matrix_size=3000\n",
    "Loop_Amount=5000\n",
    "a = torch.randn(matrix_size, matrix_size, device='cuda')\n",
    "for _ in range(Loop_Amount):  \n",
    "    b = torch.matmul(a, a)\n",
    "\n",
    "monitor_process.terminate() \n",
    "print(f\"GPU Monitoring has ended: {time.strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa1f98e9-4d94-4621-9e1a-d7181f473e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8eb90d-7dcb-4820-919a-df96806f178c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
